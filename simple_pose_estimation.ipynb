{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchvision.io import read_image\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import math\n",
    "import json\n",
    "import numpy as np\n",
    "import cv2\n",
    "from my_classes import Dataset, Net_separate, Net_combined\n",
    "from utills import rotationMatrixToEulerAngles, generate_label\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from matplotlib.ticker import PercentFormatter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "CUDA_LAUNCH_BLOCKING=1,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = 8\n",
    "params = {'batch_size' : 10,\n",
    "         'shuffle' : True,\n",
    "         'num_workers' : 1}\n",
    "max_epochs = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose train, and validation set\n",
    "\n",
    "Set 1/10 data as a validation set.\n",
    "\n",
    "    train_val : contains category agnostic traning set and val set.\n",
    "    train_val_per_category : contains per category traning set and val set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #separate train and val\n",
    "# #separate categories\n",
    "\n",
    "# #all info about imgs\n",
    "# csv_file = open(\"../../Datasets/pix3d/Pix3D/Pix3D.txt\")\n",
    "# data_f = pd.read_csv(csv_file)\n",
    "\n",
    "# #all infor about all imgs in all categories\n",
    "# dict_pix3d = np.asarray(data_f)\n",
    "\n",
    "# #categories of objects (bed, chair,and ...)\n",
    "# cats = np.unique(data_f.cat_id)\n",
    "\n",
    "# # dict for all info about all images seprated by categories. exm: {bed:[[im1_info],...],...}\n",
    "# dict_object_based = dict()#separate train and val\n",
    "# #separate categories\n",
    "\n",
    "# #all info about imgs\n",
    "# csv_file = open(\"../../Datasets/pix3d/Pix3D/Pix3D.txt\")\n",
    "# data_f = pd.read_csv(csv_file)\n",
    "\n",
    "# #all infor about all imgs in all categories\n",
    "# dict_pix3d = np.asarray(data_f)\n",
    "\n",
    "# #categories of objects (bed, chair,and ...)\n",
    "# cats = np.unique(data_f.cat_id)\n",
    "\n",
    "# # dict for all info about all images seprated by categories. exm: {bed:[[im1_info],...],...}\n",
    "# dict_object_based = dict()\n",
    "\n",
    "# #dict for train set and val set separatly for each category\n",
    "# train_val_per_category = dict()\n",
    "\n",
    "# #dict for category agnostic train val \n",
    "# train_val = dict()\n",
    "\n",
    "# #choosing train_val set\n",
    "# for i in cats:\n",
    "#     dict_object_based[i] = dict_pix3d[dict_pix3d[:, 6] == i] #get all the info for all imgs in category i\n",
    "#     partition = len(dict_object_based[i])\n",
    "#     train_val_per_category[i] = dict()\n",
    "#     train_val_per_category[i][\"train_set\"], train_val_per_category[i][\"val_set\"] = torch.utils.data.random_split(dict_object_based[i],[partition - int(partition/10), int(partition/10)] )\n",
    "#     print(i, [partition - int(partition/10), int(partition/10)] )\n",
    "#     if (len(train_val) == 0):\n",
    "#         train_val[\"train_set\"] = train_val_per_category[i][\"train_set\"]\n",
    "#         train_val[\"val_set\"] = train_val_per_category[i][\"val_set\"]\n",
    "    \n",
    "#     else:\n",
    "#         train_val[\"train_set\"] = torch.utils.data.ConcatDataset([train_val[\"train_set\"], train_val_per_category[i][\"train_set\"]])\n",
    "#         train_val[\"val_set\"] = torch.utils.data.ConcatDataset([train_val[\"val_set\"], train_val_per_category[i][\"val_set\"]])\n",
    "\n",
    "# print(\"All\", [len(train_val[\"train_set\"]), len(train_val[\"val_set\"])] )\n",
    "\n",
    "# with open('filename.pickle', 'wb') as handle:\n",
    "#     pickle.dump(train_val, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# #dict for train set and val set separatly for each category\n",
    "# train_val_per_category = dict()\n",
    "\n",
    "# #dict for category agnostic train val \n",
    "# train_val = dict()\n",
    "\n",
    "# #choosing train_val set\n",
    "# for i in cats:\n",
    "#     dict_object_based[i] = dict_pix3d[dict_pix3d[:, 6] == i] #get all the info for all imgs in category i\n",
    "#     partition = len(dict_object_based[i])\n",
    "#     train_val_per_category[i] = dict()\n",
    "#     train_val_per_category[i][\"train_set\"], train_val_per_category[i][\"val_set\"] = torch.utils.data.random_split(dict_object_based[i],[partition - int(partition/10), int(partition/10)] )\n",
    "#     print(i, [partition - int(partition/10), int(partition/10)] )\n",
    "#     if (len(train_val) == 0):\n",
    "#         train_val[\"train_set\"] = train_val_per_category[i][\"train_set\"]\n",
    "#         train_val[\"val_set\"] = train_val_per_category[i][\"val_set\"]\n",
    "    \n",
    "#     else:\n",
    "#         train_val[\"train_set\"] = torch.utils.data.ConcatDataset([train_val[\"train_set\"], train_val_per_category[i][\"train_set\"]])\n",
    "#         train_val[\"val_set\"] = torch.utils.data.ConcatDataset([train_val[\"val_set\"], train_val_per_category[i][\"val_set\"]])\n",
    "\n",
    "# print(\"All\", [len(train_val[\"train_set\"]), len(train_val[\"val_set\"])] )\n",
    "\n",
    "# with open('filename.pickle', 'wb') as handle:\n",
    "#     pickle.dump(train_val, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribution of train-val\n",
    "\n",
    "Visualizing the histogram of the train and val set based on the number of bins.\n",
    "\n",
    "All azimuth, elevation, and inplace rotation are visualized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 450x150 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open('train_test_sp.pickle', 'rb') as handle:\n",
    "    b = pickle.load(handle)\n",
    "\n",
    "train_val = b        \n",
    "#labels = np.asarray(train_val_per_category[\"chair\"][\"train_set\"])[:,-3:]\n",
    "train_labels = np.asarray(train_val[\"train_set\"])[:,-3:]\n",
    "val_labels = np.asarray(train_val[\"val_set\"])[:,-3:]\n",
    "\n",
    "x_train, y_train, z_train = train_labels[:,0], train_labels[:,1], train_labels[:,2]\n",
    "x_val, y_val, z_val= val_labels[:,0], val_labels[:,1], val_labels[:,2]\n",
    "\n",
    "# We can set the number of bins with the `bins` \n",
    "\n",
    "yaws_hist_train, binX = np.histogram(x_train, bins=n_bins, range=(0,360))\n",
    "pich_hist_train, binY = np.histogram(y_train, bins=n_bins, range=(0,360))\n",
    "roll_hist_train, binZ = np.histogram(z_train, bins=n_bins, range=(-math.pi,math.pi))\n",
    "\n",
    "yaws_hist_val, binX_val = np.histogram(x_val, bins=n_bins, range=(0,360))\n",
    "pich_hist_val, binY_val = np.histogram(y_val, bins=n_bins, range=(0,360))\n",
    "roll_hist_val, binZ_val = np.histogram(z_val, bins=n_bins, range=(-math.pi,math.pi))\n",
    "\n",
    "fig, axs = plt.subplots(2, 3 ,tight_layout=True,figsize=(9,3),dpi=50)\n",
    "\n",
    "# histogram of the train set\n",
    "axs[0,0].hist(x_train,binX )\n",
    "axs[0,1].hist(y_train,binY )\n",
    "axs[0,2].hist(z_train,binZ ) \n",
    "\n",
    "# histogram of the validation set \n",
    "axs[1,0].hist(x_val,binX_val )\n",
    "axs[1,1].hist(y_val,binY_val )\n",
    "axs[1,2].hist(z_val,binZ_val )\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Category agnostic dataset prepration \n",
    "\n",
    "Using Dataset calss in classes file, and torch.utils.data.DataLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = Dataset(np.asarray(train_val[\"train_set\"]),generate_label(train_val, n_bins, \"train_set\",\"chair\", 0 ))\n",
    "training_generator = torch.utils.data.DataLoader(training_set, **params)\n",
    "\n",
    "validation_set = Dataset(np.asarray(train_val[\"val_set\"]),generate_label(train_val, n_bins, \"val_set\", \"chair\", 0 ))\n",
    "validation_generator = torch.utils.data.DataLoader(validation_set, **params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Category level dataset prepration \n",
    "\n",
    "Using Dataset calss in classes file, and torch.utils.data.DataLoader.\n",
    "\n",
    "Please uncomment the following cell if you want to train category level. Also choose the category. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_set = Dataset(np.asarray(train_val[\"train_set\"]),generate_label(train_val, n_bins, \"train_set\",\"chair\", 1 ))\n",
    "# training_generator = torch.utils.data.DataLoader(training_set, **params)\n",
    "\n",
    "# validation_set = Dataset(np.asarray(train_val[\"val_set\"]),generate_label(train_val, n_bins, \"val_set\", \"chair\", 1 ))\n",
    "# validation_generator = torch.utils.data.DataLoader(validation_set, **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model and Loss\n",
    "\n",
    "The details of the model is in file my_classes.\n",
    "In general we have 2 main model. \n",
    "    \n",
    "    1) Net_combined() : The normal, boundries mask and rgb are all feed to a single convolution as 8 chanels. \n",
    "    2) Net_separate() :Each normal, boundries, mask, and rgb feed to a diffrent convolution layers. The features are combind to gether to estimate the pose.\n",
    "\n",
    "The loss function is cross entropy loss. I used Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#models\n",
    "\n",
    "#model = Net_separate()\n",
    "model = Net_combined(1,1,1,1)\n",
    "model.to(device)\n",
    "\n",
    "#loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001,)\n",
    "scheduler = StepLR(optimizer, step_size=3, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only rgb \n",
    "model_rgb = Net_combined(1,0,0,0)\n",
    "model_rgb.to(device)\n",
    "\n",
    "model_rgb_septate = Net_separate(1,0,0,0,device)\n",
    "model_rgb_septate.to(device)\n",
    "\n",
    "\n",
    "# rgb and mask\n",
    "model_mask = Net_combined(1,1,0,0)\n",
    "model_mask.to(device)\n",
    "\n",
    "model_mask_septate = Net_separate(1,1,0,0,device)\n",
    "model_mask_septate.to(device)\n",
    "\n",
    "\n",
    "# rgb, mask, and boundries\n",
    "model_bound = Net_combined(1,1,1,0)\n",
    "model_bound.to(device)\n",
    "\n",
    "model_bound_septate = Net_separate(1,1,1,0,device)\n",
    "model_bound_septate.to(device)\n",
    "\n",
    "\n",
    "# rgb, mask, boundries, and normal\n",
    "model_normal = Net_combined(1,1,1,1)\n",
    "model_normal.to(device)\n",
    "\n",
    "model_normal_septate = Net_separate(1,1,1,1,device)\n",
    "model_normal_septate.to(device)\n",
    "\n",
    "list_of_models = [model_rgb, model_rgb_septate, model_mask, model_mask_septate, model_bound, \n",
    "                  model_bound_septate, model_normal, model_normal_septate]\n",
    "lengthes = [3,3,4,4,5,5,8,8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Net_combined(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(1, 1))\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(64, 256, kernel_size=(7, 7), stride=(1, 1))\n",
      "  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv4): Conv2d(256, 8, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (bn4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc): Linear(in_features=968, out_features=128, bias=True)\n",
      "  (fc1): Linear(in_features=128, out_features=8, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=8, bias=True)\n",
      "  (fc3): Linear(in_features=128, out_features=8, bias=True)\n",
      ")\n",
      "Epoch [1/25], Loss: 2.0823\n",
      "Epoch [1/25], Loss: 1.6324, Val_Accuracy [(tensor(47.7046), tensor(96.2076), tensor(61.7764))]\n",
      "Epoch [2/25], Loss: 2.0437\n",
      "Epoch [2/25], Loss: 2.7467, Val_Accuracy [(tensor(50.4990), tensor(96.2076), tensor(64.3713))]\n",
      "Epoch [3/25], Loss: 2.2292\n",
      "Epoch [3/25], Loss: 1.8981, Val_Accuracy [(tensor(53.2934), tensor(96.2076), tensor(65.1697))]\n",
      "Epoch [4/25], Loss: 2.7920\n",
      "Epoch [4/25], Loss: 1.5533, Val_Accuracy [(tensor(55.8882), tensor(96.2076), tensor(63.7725))]\n",
      "Epoch [5/25], Loss: 3.2651\n",
      "Epoch [5/25], Loss: 2.3935, Val_Accuracy [(tensor(56.3872), tensor(96.2076), tensor(62.9741))]\n",
      "Epoch [6/25], Loss: 1.6857\n",
      "Epoch [6/25], Loss: 2.7999, Val_Accuracy [(tensor(56.5868), tensor(96.2076), tensor(67.4651))]\n",
      "Epoch [7/25], Loss: 0.7618\n",
      "Epoch [7/25], Loss: 1.3219, Val_Accuracy [(tensor(57.3852), tensor(96.2076), tensor(65.4691))]\n",
      "Epoch [8/25], Loss: 0.7342\n",
      "Epoch [8/25], Loss: 1.0900, Val_Accuracy [(tensor(57.1856), tensor(96.2076), tensor(67.1657))]\n",
      "Epoch [9/25], Loss: 1.2769\n",
      "Epoch [9/25], Loss: 4.5291, Val_Accuracy [(tensor(56.6866), tensor(96.5070), tensor(66.5669))]\n",
      "Epoch [10/25], Loss: 1.7924\n",
      "Epoch [10/25], Loss: 1.9146, Val_Accuracy [(tensor(56.7864), tensor(96.1078), tensor(64.9701))]\n",
      "Epoch [11/25], Loss: 3.3689\n",
      "Epoch [11/25], Loss: 1.5634, Val_Accuracy [(tensor(57.6846), tensor(96.6068), tensor(66.7665))]\n",
      "Epoch [12/25], Loss: 1.1331\n",
      "Epoch [12/25], Loss: 2.1544, Val_Accuracy [(tensor(55.5888), tensor(96.6068), tensor(65.6687))]\n",
      "Epoch [13/25], Loss: 1.5927\n",
      "Epoch [13/25], Loss: 2.2743, Val_Accuracy [(tensor(57.4850), tensor(96.1078), tensor(64.0719))]\n",
      "Epoch [14/25], Loss: 0.9797\n",
      "Epoch [14/25], Loss: 1.8320, Val_Accuracy [(tensor(57.0858), tensor(96.4072), tensor(65.0699))]\n",
      "Epoch [15/25], Loss: 1.5357\n",
      "Epoch [15/25], Loss: 1.6883, Val_Accuracy [(tensor(56.2874), tensor(96.5070), tensor(65.2695))]\n",
      "Epoch [16/25], Loss: 1.4458\n",
      "Epoch [16/25], Loss: 2.1851, Val_Accuracy [(tensor(56.6866), tensor(96.2076), tensor(65.7685))]\n",
      "Epoch [17/25], Loss: 0.5098\n",
      "Epoch [17/25], Loss: 1.9852, Val_Accuracy [(tensor(57.4850), tensor(96.9062), tensor(62.3752))]\n",
      "Epoch [18/25], Loss: 1.2919\n",
      "Epoch [18/25], Loss: 4.0161, Val_Accuracy [(tensor(56.5868), tensor(96.4072), tensor(64.3713))]\n",
      "Epoch [19/25], Loss: 1.7730\n",
      "Epoch [19/25], Loss: 2.5941, Val_Accuracy [(tensor(56.4870), tensor(96.3074), tensor(63.3733))]\n",
      "Epoch [20/25], Loss: 0.2216\n",
      "Epoch [20/25], Loss: 2.0567, Val_Accuracy [(tensor(55.4890), tensor(96.4072), tensor(64.2715))]\n",
      "Epoch [21/25], Loss: 0.9302\n",
      "Epoch [21/25], Loss: 2.8184, Val_Accuracy [(tensor(57.9840), tensor(96.5070), tensor(62.8743))]\n",
      "Epoch [22/25], Loss: 0.9747\n",
      "Epoch [22/25], Loss: 2.9528, Val_Accuracy [(tensor(56.0878), tensor(96.6068), tensor(63.6727))]\n",
      "Epoch [23/25], Loss: 1.1819\n",
      "Epoch [23/25], Loss: 3.8080, Val_Accuracy [(tensor(57.3852), tensor(96.2076), tensor(62.4750))]\n",
      "Epoch [24/25], Loss: 1.0631\n",
      "Epoch [24/25], Loss: 8.1587, Val_Accuracy [(tensor(56.3872), tensor(96.6068), tensor(62.4750))]\n",
      "Epoch [25/25], Loss: 0.1073\n",
      "Epoch [25/25], Loss: 0.7275, Val_Accuracy [(tensor(56.8862), tensor(96.3074), tensor(61.1776))]\n",
      "1\n",
      "Net_separate(\n",
      "  (conv1_rgb): Conv2d(3, 32, kernel_size=(7, 7), stride=(1, 1))\n",
      "  (bn1_rgb): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2_rgb): Conv2d(32, 64, kernel_size=(7, 7), stride=(1, 1))\n",
      "  (bn2_rgb): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv4_rgb): Conv2d(64, 8, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (bn4_rgb): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc_rgb): Linear(in_features=4608, out_features=128, bias=True)\n",
      "  (conv1_mask): Conv2d(1, 32, kernel_size=(7, 7), stride=(1, 1))\n",
      "  (bn1_mask): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2_mask): Conv2d(32, 64, kernel_size=(7, 7), stride=(1, 1))\n",
      "  (bn2_mask): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv4_mask): Conv2d(64, 8, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (bn4_mask): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc_mask): Linear(in_features=4608, out_features=128, bias=True)\n",
      "  (conv1_bound): Conv2d(1, 32, kernel_size=(7, 7), stride=(1, 1))\n",
      "  (bn1_bound): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2_bound): Conv2d(32, 64, kernel_size=(7, 7), stride=(1, 1))\n",
      "  (bn2_bound): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv4_bound): Conv2d(64, 8, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (bn4_bound): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc_bound): Linear(in_features=4608, out_features=128, bias=True)\n",
      "  (conv1_normal): Conv2d(3, 32, kernel_size=(7, 7), stride=(1, 1))\n",
      "  (bn1_normal): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2_normal): Conv2d(32, 64, kernel_size=(7, 7), stride=(1, 1))\n",
      "  (bn2_normal): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv4_normal): Conv2d(64, 8, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (bn4_normal): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc_normal): Linear(in_features=4608, out_features=128, bias=True)\n",
      "  (fc1): Linear(in_features=128, out_features=8, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=8, bias=True)\n",
      "  (fc3): Linear(in_features=128, out_features=8, bias=True)\n",
      ")\n",
      "Epoch [1/25], Loss: 4.5592\n",
      "Epoch [1/25], Loss: 4.4165, Val_Accuracy [(tensor(39.9202), tensor(96.2076), tensor(61.6766))]\n",
      "Epoch [2/25], Loss: 3.4452\n",
      "Epoch [2/25], Loss: 4.6349, Val_Accuracy [(tensor(43.1138), tensor(96.2076), tensor(64.1717))]\n",
      "Epoch [3/25], Loss: 3.5801\n",
      "Epoch [3/25], Loss: 4.8038, Val_Accuracy [(tensor(46.0080), tensor(96.1078), tensor(62.1756))]\n",
      "Epoch [4/25], Loss: 5.3394\n",
      "Epoch [4/25], Loss: 4.8838, Val_Accuracy [(tensor(46.7066), tensor(96.0080), tensor(64.3713))]\n",
      "Epoch [5/25], Loss: 4.1092\n",
      "Epoch [5/25], Loss: 3.7561, Val_Accuracy [(tensor(43.2136), tensor(96.1078), tensor(69.3613))]\n",
      "Epoch [6/25], Loss: 4.0379\n",
      "Epoch [6/25], Loss: 5.0551, Val_Accuracy [(tensor(43.5130), tensor(95.9082), tensor(66.8663))]\n",
      "Epoch [7/25], Loss: 4.1982\n",
      "Epoch [7/25], Loss: 4.6583, Val_Accuracy [(tensor(47.1058), tensor(96.1078), tensor(66.6667))]\n",
      "Epoch [8/25], Loss: 4.1987\n",
      "Epoch [8/25], Loss: 4.9048, Val_Accuracy [(tensor(45.5090), tensor(96.2076), tensor(68.4631))]\n",
      "Epoch [9/25], Loss: 4.6647\n",
      "Epoch [9/25], Loss: 3.4248, Val_Accuracy [(tensor(49.1018), tensor(96.2076), tensor(65.7685))]\n",
      "Epoch [10/25], Loss: 3.8806\n",
      "Epoch [10/25], Loss: 4.3311, Val_Accuracy [(tensor(49.8004), tensor(96.2076), tensor(67.7645))]\n",
      "Epoch [11/25], Loss: 2.8757\n",
      "Epoch [11/25], Loss: 3.5204, Val_Accuracy [(tensor(49.7006), tensor(96.2076), tensor(67.3653))]\n",
      "Epoch [12/25], Loss: 1.2716\n",
      "Epoch [12/25], Loss: 2.4218, Val_Accuracy [(tensor(47.8044), tensor(96.2076), tensor(67.1657))]\n",
      "Epoch [13/25], Loss: 1.8413\n",
      "Epoch [13/25], Loss: 2.1310, Val_Accuracy [(tensor(48.6028), tensor(96.2076), tensor(68.0639))]\n",
      "Epoch [14/25], Loss: 2.3613\n",
      "Epoch [14/25], Loss: 5.2555, Val_Accuracy [(tensor(48.8024), tensor(96.2076), tensor(66.3673))]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/25], Loss: 1.1517\n",
      "Epoch [15/25], Loss: 2.6784, Val_Accuracy [(tensor(51.0978), tensor(96.2076), tensor(68.2635))]\n",
      "Epoch [16/25], Loss: 1.0178\n",
      "Epoch [16/25], Loss: 1.1541, Val_Accuracy [(tensor(48.7026), tensor(96.4072), tensor(67.5649))]\n",
      "Epoch [17/25], Loss: 0.9070\n",
      "Epoch [17/25], Loss: 2.1125, Val_Accuracy [(tensor(50.0998), tensor(95.6088), tensor(67.6647))]\n",
      "Epoch [18/25], Loss: 3.9408\n",
      "Epoch [18/25], Loss: 2.9622, Val_Accuracy [(tensor(51.6966), tensor(96.2076), tensor(66.8663))]\n",
      "Epoch [19/25], Loss: 1.2094\n",
      "Epoch [19/25], Loss: 2.8268, Val_Accuracy [(tensor(51.1976), tensor(95.5090), tensor(65.3693))]\n",
      "Epoch [20/25], Loss: 1.2644\n",
      "Epoch [20/25], Loss: 2.9609, Val_Accuracy [(tensor(52.1956), tensor(96.4072), tensor(65.7685))]\n",
      "Epoch [21/25], Loss: 1.6219\n",
      "Epoch [21/25], Loss: 4.0954, Val_Accuracy [(tensor(50.3992), tensor(96.4072), tensor(65.5689))]\n",
      "Epoch [22/25], Loss: 1.6992\n",
      "Epoch [22/25], Loss: 5.0313, Val_Accuracy [(tensor(50.4990), tensor(95.8084), tensor(65.2695))]\n",
      "Epoch [23/25], Loss: 1.7235\n",
      "Epoch [23/25], Loss: 2.1441, Val_Accuracy [(tensor(49.8004), tensor(96.0080), tensor(66.7665))]\n",
      "Epoch [24/25], Loss: 0.5405\n",
      "Epoch [24/25], Loss: 1.2390, Val_Accuracy [(tensor(50.6986), tensor(96.3074), tensor(65.0699))]\n",
      "Epoch [25/25], Loss: 1.5763\n",
      "Epoch [25/25], Loss: 6.9473, Val_Accuracy [(tensor(50.7984), tensor(96.0080), tensor(65.8683))]\n",
      "2\n",
      "Net_combined(\n",
      "  (conv1): Conv2d(4, 64, kernel_size=(7, 7), stride=(1, 1))\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(64, 256, kernel_size=(7, 7), stride=(1, 1))\n",
      "  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv4): Conv2d(256, 8, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (bn4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc): Linear(in_features=968, out_features=128, bias=True)\n",
      "  (fc1): Linear(in_features=128, out_features=8, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=8, bias=True)\n",
      "  (fc3): Linear(in_features=128, out_features=8, bias=True)\n",
      ")\n",
      "Epoch [1/25], Loss: 4.1768\n",
      "Epoch [1/25], Loss: 4.2190, Val_Accuracy [(tensor(57.8842), tensor(96.0080), tensor(60.5788))]\n",
      "Epoch [2/25], Loss: 3.4310\n",
      "Epoch [2/25], Loss: 3.4405, Val_Accuracy [(tensor(64.3713), tensor(86.6267), tensor(66.1677))]\n",
      "Epoch [3/25], Loss: 3.5094\n",
      "Epoch [3/25], Loss: 3.2316, Val_Accuracy [(tensor(69.5609), tensor(86.4271), tensor(68.6627))]\n",
      "Epoch [4/25], Loss: 3.1405\n",
      "Epoch [4/25], Loss: 4.6393, Val_Accuracy [(tensor(70.1597), tensor(91.8164), tensor(65.9681))]\n",
      "Epoch [5/25], Loss: 3.5211\n",
      "Epoch [5/25], Loss: 3.0071, Val_Accuracy [(tensor(72.4551), tensor(93.4132), tensor(68.5629))]\n",
      "Epoch [6/25], Loss: 3.4219\n",
      "Epoch [6/25], Loss: 3.1368, Val_Accuracy [(tensor(70.9581), tensor(95.0100), tensor(68.7625))]\n",
      "Epoch [7/25], Loss: 2.8801\n",
      "Epoch [7/25], Loss: 2.7440, Val_Accuracy [(tensor(71.4571), tensor(94.2116), tensor(67.1657))]\n",
      "Epoch [8/25], Loss: 3.7559\n",
      "Epoch [8/25], Loss: 4.3688, Val_Accuracy [(tensor(74.3513), tensor(88.5230), tensor(68.3633))]\n",
      "Epoch [9/25], Loss: 2.6276\n",
      "Epoch [9/25], Loss: 3.0513, Val_Accuracy [(tensor(74.3513), tensor(88.5230), tensor(67.8643))]\n",
      "Epoch [10/25], Loss: 2.9033\n",
      "Epoch [10/25], Loss: 3.2484, Val_Accuracy [(tensor(72.6547), tensor(92.1158), tensor(66.1677))]\n",
      "Epoch [11/25], Loss: 2.8930\n",
      "Epoch [11/25], Loss: 3.2807, Val_Accuracy [(tensor(72.9541), tensor(90.6188), tensor(67.4651))]\n",
      "Epoch [12/25], Loss: 2.4136\n",
      "Epoch [12/25], Loss: 2.9257, Val_Accuracy [(tensor(73.9521), tensor(91.4172), tensor(67.9641))]\n",
      "Epoch [13/25], Loss: 3.3984\n",
      "Epoch [13/25], Loss: 2.1282, Val_Accuracy [(tensor(73.1537), tensor(90.3194), tensor(67.1657))]\n",
      "Epoch [14/25], Loss: 3.0270\n",
      "Epoch [14/25], Loss: 3.4654, Val_Accuracy [(tensor(72.6547), tensor(92.1158), tensor(65.2695))]\n",
      "Epoch [15/25], Loss: 2.5501\n",
      "Epoch [15/25], Loss: 3.3327, Val_Accuracy [(tensor(72.8543), tensor(91.6168), tensor(66.5669))]\n",
      "Epoch [16/25], Loss: 2.4053\n",
      "Epoch [16/25], Loss: 5.3835, Val_Accuracy [(tensor(72.5549), tensor(90.5190), tensor(64.7705))]\n",
      "Epoch [17/25], Loss: 2.7549\n",
      "Epoch [17/25], Loss: 2.5739, Val_Accuracy [(tensor(70.6587), tensor(93.8124), tensor(65.8683))]\n",
      "Epoch [18/25], Loss: 3.1046\n",
      "Epoch [18/25], Loss: 3.0009, Val_Accuracy [(tensor(72.1557), tensor(88.3234), tensor(66.9661))]\n",
      "Epoch [19/25], Loss: 2.3342\n",
      "Epoch [19/25], Loss: 2.5082, Val_Accuracy [(tensor(71.7565), tensor(95.3094), tensor(65.6687))]\n",
      "Epoch [20/25], Loss: 2.7017\n",
      "Epoch [20/25], Loss: 5.0470, Val_Accuracy [(tensor(73.5529), tensor(94.0120), tensor(65.4691))]\n",
      "Epoch [21/25], Loss: 2.4653\n",
      "Epoch [21/25], Loss: 3.3883, Val_Accuracy [(tensor(71.9561), tensor(91.7166), tensor(66.3673))]\n",
      "Epoch [22/25], Loss: 1.0175\n",
      "Epoch [22/25], Loss: 3.3280, Val_Accuracy [(tensor(70.0599), tensor(96.7066), tensor(67.6647))]\n",
      "Epoch [23/25], Loss: 0.1621\n",
      "Epoch [23/25], Loss: 1.5415, Val_Accuracy [(tensor(71.9561), tensor(96.6068), tensor(66.7665))]\n",
      "Epoch [24/25], Loss: 0.7664\n",
      "Epoch [24/25], Loss: 1.0603, Val_Accuracy [(tensor(72.0559), tensor(96.8064), tensor(65.8683))]\n",
      "Epoch [25/25], Loss: 1.1113\n",
      "Epoch [25/25], Loss: 1.4829, Val_Accuracy [(tensor(70.6587), tensor(95.7086), tensor(65.3693))]\n",
      "3\n",
      "Net_separate(\n",
      "  (conv1_rgb): Conv2d(3, 32, kernel_size=(7, 7), stride=(1, 1))\n",
      "  (bn1_rgb): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2_rgb): Conv2d(32, 64, kernel_size=(7, 7), stride=(1, 1))\n",
      "  (bn2_rgb): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv4_rgb): Conv2d(64, 8, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (bn4_rgb): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc_rgb): Linear(in_features=4608, out_features=128, bias=True)\n",
      "  (conv1_mask): Conv2d(1, 32, kernel_size=(7, 7), stride=(1, 1))\n",
      "  (bn1_mask): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2_mask): Conv2d(32, 64, kernel_size=(7, 7), stride=(1, 1))\n",
      "  (bn2_mask): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv4_mask): Conv2d(64, 8, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (bn4_mask): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc_mask): Linear(in_features=4608, out_features=128, bias=True)\n",
      "  (conv1_bound): Conv2d(1, 32, kernel_size=(7, 7), stride=(1, 1))\n",
      "  (bn1_bound): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2_bound): Conv2d(32, 64, kernel_size=(7, 7), stride=(1, 1))\n",
      "  (bn2_bound): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv4_bound): Conv2d(64, 8, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (bn4_bound): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc_bound): Linear(in_features=4608, out_features=128, bias=True)\n",
      "  (conv1_normal): Conv2d(3, 32, kernel_size=(7, 7), stride=(1, 1))\n",
      "  (bn1_normal): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2_normal): Conv2d(32, 64, kernel_size=(7, 7), stride=(1, 1))\n",
      "  (bn2_normal): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv4_normal): Conv2d(64, 8, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (bn4_normal): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc_normal): Linear(in_features=4608, out_features=128, bias=True)\n",
      "  (fc1): Linear(in_features=256, out_features=8, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=8, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=8, bias=True)\n",
      ")\n",
      "Epoch [1/25], Loss: 3.5103\n",
      "Epoch [1/25], Loss: 2.7074, Val_Accuracy [(tensor(35.0299), tensor(96.2076), tensor(49.5010))]\n",
      "Epoch [2/25], Loss: 3.0571\n",
      "Epoch [2/25], Loss: 1.9181, Val_Accuracy [(tensor(29.9401), tensor(96.2076), tensor(52.5948))]\n",
      "Epoch [3/25], Loss: 3.6737\n",
      "Epoch [3/25], Loss: 1.8899, Val_Accuracy [(tensor(34.8303), tensor(96.2076), tensor(50.7984))]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/25], Loss: 3.4502\n",
      "Epoch [4/25], Loss: 1.7894, Val_Accuracy [(tensor(33.9321), tensor(96.2076), tensor(49.6008))]\n",
      "Epoch [5/25], Loss: 4.2003\n",
      "Epoch [5/25], Loss: 1.8762, Val_Accuracy [(tensor(36.4271), tensor(96.2076), tensor(48.7026))]\n",
      "Epoch [6/25], Loss: 5.0632\n",
      "Epoch [6/25], Loss: 3.7114, Val_Accuracy [(tensor(36.3273), tensor(96.2076), tensor(52.6946))]\n",
      "Epoch [7/25], Loss: 2.8538\n",
      "Epoch [7/25], Loss: 1.7179, Val_Accuracy [(tensor(36.4271), tensor(96.2076), tensor(51.3972))]\n",
      "Epoch [8/25], Loss: 1.7946\n",
      "Epoch [8/25], Loss: 3.7325, Val_Accuracy [(tensor(34.1317), tensor(96.2076), tensor(49.6008))]\n",
      "Epoch [9/25], Loss: 2.3604\n",
      "Epoch [9/25], Loss: 2.9014, Val_Accuracy [(tensor(37.5250), tensor(96.2076), tensor(51.0978))]\n",
      "Epoch [10/25], Loss: 3.9618\n",
      "Epoch [10/25], Loss: 2.2390, Val_Accuracy [(tensor(36.8263), tensor(96.2076), tensor(48.7026))]\n",
      "Epoch [11/25], Loss: 2.0922\n",
      "Epoch [11/25], Loss: 1.4230, Val_Accuracy [(tensor(36.4271), tensor(96.2076), tensor(50.3992))]\n",
      "Epoch [12/25], Loss: 2.1688\n",
      "Epoch [12/25], Loss: 3.0548, Val_Accuracy [(tensor(36.7265), tensor(96.2076), tensor(50.1996))]\n",
      "Epoch [13/25], Loss: 1.8872\n",
      "Epoch [13/25], Loss: 1.6920, Val_Accuracy [(tensor(36.9261), tensor(96.2076), tensor(48.9022))]\n",
      "Epoch [14/25], Loss: 2.3157\n",
      "Epoch [14/25], Loss: 2.5247, Val_Accuracy [(tensor(35.1297), tensor(96.2076), tensor(50.5988))]\n",
      "Epoch [15/25], Loss: 2.4713\n",
      "Epoch [15/25], Loss: 4.3108, Val_Accuracy [(tensor(34.8303), tensor(96.2076), tensor(51.7964))]\n",
      "Epoch [16/25], Loss: 3.2673\n",
      "Epoch [16/25], Loss: 1.6592, Val_Accuracy [(tensor(35.7285), tensor(96.2076), tensor(51.0978))]\n",
      "Epoch [17/25], Loss: 2.3742\n",
      "Epoch [17/25], Loss: 1.9399, Val_Accuracy [(tensor(35.1297), tensor(96.2076), tensor(50.9980))]\n",
      "Epoch [18/25], Loss: 1.9400\n",
      "Epoch [18/25], Loss: 1.8389, Val_Accuracy [(tensor(36.7265), tensor(96.2076), tensor(50.7984))]\n",
      "Epoch [19/25], Loss: 3.4248\n",
      "Epoch [19/25], Loss: 4.4057, Val_Accuracy [(tensor(36.7265), tensor(96.2076), tensor(48.4032))]\n",
      "Epoch [20/25], Loss: 1.7523\n",
      "Epoch [20/25], Loss: 2.7392, Val_Accuracy [(tensor(36.6267), tensor(96.2076), tensor(49.8004))]\n",
      "Epoch [21/25], Loss: 2.7772\n",
      "Epoch [21/25], Loss: 2.2615, Val_Accuracy [(tensor(35.9281), tensor(96.2076), tensor(51.8962))]\n",
      "Epoch [22/25], Loss: 3.1638\n",
      "Epoch [22/25], Loss: 2.6617, Val_Accuracy [(tensor(35.9281), tensor(96.2076), tensor(52.8942))]\n",
      "Epoch [23/25], Loss: 5.9847\n",
      "Epoch [23/25], Loss: 2.7328, Val_Accuracy [(tensor(36.5269), tensor(96.2076), tensor(50.6986))]\n",
      "Epoch [24/25], Loss: 2.7946\n",
      "Epoch [24/25], Loss: 1.8396, Val_Accuracy [(tensor(36.9261), tensor(96.2076), tensor(50.9980))]\n",
      "Epoch [25/25], Loss: 2.0034\n",
      "Epoch [25/25], Loss: 4.6033, Val_Accuracy [(tensor(36.9261), tensor(96.2076), tensor(54.1916))]\n",
      "4\n",
      "Net_combined(\n",
      "  (conv1): Conv2d(5, 64, kernel_size=(7, 7), stride=(1, 1))\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(64, 256, kernel_size=(7, 7), stride=(1, 1))\n",
      "  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv4): Conv2d(256, 8, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (bn4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc): Linear(in_features=968, out_features=128, bias=True)\n",
      "  (fc1): Linear(in_features=128, out_features=8, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=8, bias=True)\n",
      "  (fc3): Linear(in_features=128, out_features=8, bias=True)\n",
      ")\n",
      "Epoch [1/25], Loss: 4.6743\n",
      "Epoch [1/25], Loss: 5.6179, Val_Accuracy [(tensor(54.2914), tensor(96.2076), tensor(62.6747))]\n",
      "Epoch [2/25], Loss: 4.6957\n",
      "Epoch [2/25], Loss: 4.2589, Val_Accuracy [(tensor(54.8902), tensor(96.2076), tensor(68.4631))]\n",
      "Epoch [3/25], Loss: 4.5732\n",
      "Epoch [3/25], Loss: 3.9644, Val_Accuracy [(tensor(55.5888), tensor(96.2076), tensor(66.0679))]\n",
      "Epoch [4/25], Loss: 2.3624\n",
      "Epoch [4/25], Loss: 1.9752, Val_Accuracy [(tensor(56.1876), tensor(96.2076), tensor(66.5669))]\n",
      "Epoch [5/25], Loss: 2.5401\n",
      "Epoch [5/25], Loss: 1.3279, Val_Accuracy [(tensor(55.6886), tensor(96.2076), tensor(67.8643))]\n",
      "Epoch [6/25], Loss: 3.1653\n",
      "Epoch [6/25], Loss: 2.3751, Val_Accuracy [(tensor(56.1876), tensor(96.2076), tensor(69.5609))]\n",
      "Epoch [7/25], Loss: 2.0493\n",
      "Epoch [7/25], Loss: 1.3551, Val_Accuracy [(tensor(56.2874), tensor(96.2076), tensor(72.2555))]\n",
      "Epoch [8/25], Loss: 1.4623\n",
      "Epoch [8/25], Loss: 0.4902, Val_Accuracy [(tensor(57.0858), tensor(97.0060), tensor(70.8583))]\n",
      "Epoch [9/25], Loss: 2.8090\n",
      "Epoch [9/25], Loss: 2.3510, Val_Accuracy [(tensor(56.7864), tensor(96.9062), tensor(68.2635))]\n",
      "Epoch [10/25], Loss: 2.0554\n",
      "Epoch [10/25], Loss: 1.8260, Val_Accuracy [(tensor(56.5868), tensor(97.3054), tensor(67.0659))]\n",
      "Epoch [11/25], Loss: 2.0518\n",
      "Epoch [11/25], Loss: 1.2200, Val_Accuracy [(tensor(57.0858), tensor(96.5070), tensor(69.0619))]\n",
      "Epoch [12/25], Loss: 2.5829\n",
      "Epoch [12/25], Loss: 1.7288, Val_Accuracy [(tensor(56.1876), tensor(97.3054), tensor(68.4631))]\n",
      "Epoch [13/25], Loss: 0.9863\n",
      "Epoch [13/25], Loss: 3.1158, Val_Accuracy [(tensor(54.6906), tensor(97.1058), tensor(70.7585))]\n",
      "Epoch [14/25], Loss: 3.8715\n",
      "Epoch [14/25], Loss: 1.7873, Val_Accuracy [(tensor(56.0878), tensor(97.1058), tensor(67.1657))]\n",
      "Epoch [15/25], Loss: 1.7176\n",
      "Epoch [15/25], Loss: 0.3820, Val_Accuracy [(tensor(55.5888), tensor(97.1058), tensor(67.8643))]\n",
      "Epoch [16/25], Loss: 0.9819\n",
      "Epoch [16/25], Loss: 1.0575, Val_Accuracy [(tensor(56.7864), tensor(97.1058), tensor(68.5629))]\n",
      "Epoch [17/25], Loss: 0.8470\n",
      "Epoch [17/25], Loss: 2.6236, Val_Accuracy [(tensor(57.3852), tensor(95.8084), tensor(68.3633))]\n",
      "Epoch [18/25], Loss: 2.0342\n",
      "Epoch [18/25], Loss: 0.4395, Val_Accuracy [(tensor(56.3872), tensor(97.0060), tensor(66.6667))]\n",
      "Epoch [19/25], Loss: 1.2658\n",
      "Epoch [19/25], Loss: 4.1534, Val_Accuracy [(tensor(55.1896), tensor(96.9062), tensor(68.3633))]\n",
      "Epoch [20/25], Loss: 0.7995\n",
      "Epoch [20/25], Loss: 8.1410, Val_Accuracy [(tensor(55.6886), tensor(96.9062), tensor(67.8643))]\n",
      "Epoch [21/25], Loss: 2.4606\n",
      "Epoch [21/25], Loss: 2.8430, Val_Accuracy [(tensor(55.5888), tensor(95.3094), tensor(67.9641))]\n",
      "Epoch [22/25], Loss: 0.5967\n",
      "Epoch [22/25], Loss: 2.3980, Val_Accuracy [(tensor(56.1876), tensor(97.3054), tensor(68.3633))]\n",
      "Epoch [23/25], Loss: 1.7554\n",
      "Epoch [23/25], Loss: 1.1251, Val_Accuracy [(tensor(56.7864), tensor(96.8064), tensor(67.7645))]\n",
      "Epoch [24/25], Loss: 1.2422\n",
      "Epoch [24/25], Loss: 2.8386, Val_Accuracy [(tensor(55.9880), tensor(97.3054), tensor(67.8643))]\n",
      "Epoch [25/25], Loss: 0.5563\n",
      "Epoch [25/25], Loss: 1.4774, Val_Accuracy [(tensor(57.1856), tensor(96.8064), tensor(66.6667))]\n",
      "5\n",
      "Net_separate(\n",
      "  (conv1_rgb): Conv2d(3, 32, kernel_size=(7, 7), stride=(1, 1))\n",
      "  (bn1_rgb): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2_rgb): Conv2d(32, 64, kernel_size=(7, 7), stride=(1, 1))\n",
      "  (bn2_rgb): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv4_rgb): Conv2d(64, 8, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (bn4_rgb): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc_rgb): Linear(in_features=4608, out_features=128, bias=True)\n",
      "  (conv1_mask): Conv2d(1, 32, kernel_size=(7, 7), stride=(1, 1))\n",
      "  (bn1_mask): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2_mask): Conv2d(32, 64, kernel_size=(7, 7), stride=(1, 1))\n",
      "  (bn2_mask): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv4_mask): Conv2d(64, 8, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (bn4_mask): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc_mask): Linear(in_features=4608, out_features=128, bias=True)\n",
      "  (conv1_bound): Conv2d(1, 32, kernel_size=(7, 7), stride=(1, 1))\n",
      "  (bn1_bound): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2_bound): Conv2d(32, 64, kernel_size=(7, 7), stride=(1, 1))\n",
      "  (bn2_bound): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv4_bound): Conv2d(64, 8, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (bn4_bound): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc_bound): Linear(in_features=4608, out_features=128, bias=True)\n",
      "  (conv1_normal): Conv2d(3, 32, kernel_size=(7, 7), stride=(1, 1))\n",
      "  (bn1_normal): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2_normal): Conv2d(32, 64, kernel_size=(7, 7), stride=(1, 1))\n",
      "  (bn2_normal): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv4_normal): Conv2d(64, 8, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (bn4_normal): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc_normal): Linear(in_features=4608, out_features=128, bias=True)\n",
      "  (fc1): Linear(in_features=384, out_features=8, bias=True)\n",
      "  (fc2): Linear(in_features=384, out_features=8, bias=True)\n",
      "  (fc3): Linear(in_features=384, out_features=8, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25], Loss: 3.7770\n",
      "Epoch [1/25], Loss: 4.7248, Val_Accuracy [(tensor(34.8303), tensor(96.2076), tensor(48.8024))]\n",
      "Epoch [2/25], Loss: 2.6194\n",
      "Epoch [2/25], Loss: 4.2031, Val_Accuracy [(tensor(34.0319), tensor(96.2076), tensor(50.6986))]\n",
      "Epoch [3/25], Loss: 4.0081\n",
      "Epoch [3/25], Loss: 3.3062, Val_Accuracy [(tensor(37.4251), tensor(96.2076), tensor(50.4990))]\n",
      "Epoch [4/25], Loss: 2.8763\n",
      "Epoch [4/25], Loss: 2.8103, Val_Accuracy [(tensor(36.1277), tensor(96.2076), tensor(49.5010))]\n",
      "Epoch [5/25], Loss: 2.0387\n",
      "Epoch [5/25], Loss: 2.2011, Val_Accuracy [(tensor(37.7246), tensor(96.2076), tensor(51.9960))]\n",
      "Epoch [6/25], Loss: 1.8707\n",
      "Epoch [6/25], Loss: 2.1164, Val_Accuracy [(tensor(37.0259), tensor(96.2076), tensor(48.5030))]\n",
      "Epoch [7/25], Loss: 2.9696\n",
      "Epoch [7/25], Loss: 3.4875, Val_Accuracy [(tensor(38.6228), tensor(96.2076), tensor(53.7924))]\n",
      "Epoch [8/25], Loss: 3.3710\n",
      "Epoch [8/25], Loss: 2.1429, Val_Accuracy [(tensor(33.2335), tensor(96.2076), tensor(50.4990))]\n",
      "Epoch [9/25], Loss: 2.4836\n",
      "Epoch [9/25], Loss: 1.9933, Val_Accuracy [(tensor(38.5230), tensor(96.2076), tensor(48.8024))]\n",
      "Epoch [10/25], Loss: 3.4579\n",
      "Epoch [10/25], Loss: 2.2436, Val_Accuracy [(tensor(36.8263), tensor(96.2076), tensor(50.0998))]\n",
      "Epoch [11/25], Loss: 2.3581\n",
      "Epoch [11/25], Loss: 2.1482, Val_Accuracy [(tensor(38.1238), tensor(96.2076), tensor(51.4970))]\n",
      "Epoch [12/25], Loss: 1.3144\n",
      "Epoch [12/25], Loss: 2.6799, Val_Accuracy [(tensor(36.1277), tensor(96.2076), tensor(54.2914))]\n",
      "Epoch [13/25], Loss: 1.8941\n",
      "Epoch [13/25], Loss: 1.8129, Val_Accuracy [(tensor(38.4232), tensor(96.2076), tensor(51.2974))]\n",
      "Epoch [14/25], Loss: 2.6608\n",
      "Epoch [14/25], Loss: 1.7658, Val_Accuracy [(tensor(37.6248), tensor(96.2076), tensor(50.6986))]\n",
      "Epoch [15/25], Loss: 1.6872\n",
      "Epoch [15/25], Loss: 2.8465, Val_Accuracy [(tensor(35.1297), tensor(96.2076), tensor(52.8942))]\n",
      "Epoch [16/25], Loss: 3.1584\n",
      "Epoch [16/25], Loss: 1.9639, Val_Accuracy [(tensor(39.0220), tensor(96.2076), tensor(50.7984))]\n",
      "Epoch [17/25], Loss: 2.7084\n",
      "Epoch [17/25], Loss: 1.5616, Val_Accuracy [(tensor(37.9242), tensor(96.2076), tensor(50.6986))]\n",
      "Epoch [18/25], Loss: 1.6921\n",
      "Epoch [18/25], Loss: 1.8950, Val_Accuracy [(tensor(38.2236), tensor(96.2076), tensor(48.8024))]\n",
      "Epoch [19/25], Loss: 3.0875\n",
      "Epoch [19/25], Loss: 1.7725, Val_Accuracy [(tensor(39.2216), tensor(96.2076), tensor(49.9002))]\n",
      "Epoch [20/25], Loss: 2.6303\n",
      "Epoch [20/25], Loss: 3.5017, Val_Accuracy [(tensor(36.8263), tensor(96.2076), tensor(50.9980))]\n",
      "Epoch [21/25], Loss: 2.2365\n",
      "Epoch [21/25], Loss: 1.2439, Val_Accuracy [(tensor(38.4232), tensor(96.2076), tensor(51.6966))]\n",
      "Epoch [22/25], Loss: 2.4846\n",
      "Epoch [22/25], Loss: 3.0052, Val_Accuracy [(tensor(38.0240), tensor(96.2076), tensor(53.3932))]\n",
      "Epoch [23/25], Loss: 1.7570\n",
      "Epoch [23/25], Loss: 2.6956, Val_Accuracy [(tensor(37.8244), tensor(96.2076), tensor(50.))]\n",
      "Epoch [24/25], Loss: 1.7443\n",
      "Epoch [24/25], Loss: 4.7383, Val_Accuracy [(tensor(38.1238), tensor(96.2076), tensor(50.))]\n",
      "Epoch [25/25], Loss: 1.8965\n",
      "Epoch [25/25], Loss: 1.1437, Val_Accuracy [(tensor(38.1238), tensor(96.2076), tensor(51.7964))]\n",
      "6\n",
      "Net_combined(\n",
      "  (conv1): Conv2d(8, 64, kernel_size=(7, 7), stride=(1, 1))\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(64, 256, kernel_size=(7, 7), stride=(1, 1))\n",
      "  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv4): Conv2d(256, 8, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (bn4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc): Linear(in_features=968, out_features=128, bias=True)\n",
      "  (fc1): Linear(in_features=128, out_features=8, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=8, bias=True)\n",
      "  (fc3): Linear(in_features=128, out_features=8, bias=True)\n",
      ")\n",
      "Epoch [1/25], Loss: 2.0337\n",
      "Epoch [1/25], Loss: 1.7140, Val_Accuracy [(tensor(53.1936), tensor(96.2076), tensor(63.3733))]\n",
      "Epoch [2/25], Loss: 2.1207\n",
      "Epoch [2/25], Loss: 2.9826, Val_Accuracy [(tensor(55.2894), tensor(96.2076), tensor(65.3693))]\n",
      "Epoch [3/25], Loss: 1.5418\n",
      "Epoch [3/25], Loss: 1.7972, Val_Accuracy [(tensor(59.4810), tensor(96.2076), tensor(66.2675))]\n",
      "Epoch [4/25], Loss: 1.8218\n",
      "Epoch [4/25], Loss: 2.6987, Val_Accuracy [(tensor(61.8762), tensor(96.5070), tensor(69.3613))]\n",
      "Epoch [5/25], Loss: 0.7011\n",
      "Epoch [5/25], Loss: 1.6861, Val_Accuracy [(tensor(61.6766), tensor(96.6068), tensor(68.1637))]\n",
      "Epoch [6/25], Loss: 1.2860\n",
      "Epoch [6/25], Loss: 2.0232, Val_Accuracy [(tensor(62.9741), tensor(96.6068), tensor(69.5609))]\n",
      "Epoch [7/25], Loss: 2.0693\n",
      "Epoch [7/25], Loss: 1.7264, Val_Accuracy [(tensor(62.6747), tensor(96.5070), tensor(69.7605))]\n",
      "Epoch [8/25], Loss: 1.5654\n",
      "Epoch [8/25], Loss: 0.7209, Val_Accuracy [(tensor(64.2715), tensor(96.1078), tensor(69.3613))]\n",
      "Epoch [9/25], Loss: 1.2629\n",
      "Epoch [9/25], Loss: 3.9289, Val_Accuracy [(tensor(63.1737), tensor(96.9062), tensor(69.8603))]\n",
      "Epoch [10/25], Loss: 2.2564\n",
      "Epoch [10/25], Loss: 0.6883, Val_Accuracy [(tensor(64.9701), tensor(97.0060), tensor(69.2615))]\n",
      "Epoch [11/25], Loss: 2.5247\n",
      "Epoch [11/25], Loss: 1.7465, Val_Accuracy [(tensor(65.5689), tensor(96.9062), tensor(70.8583))]\n",
      "Epoch [12/25], Loss: 1.2636\n",
      "Epoch [12/25], Loss: 0.7579, Val_Accuracy [(tensor(64.6707), tensor(97.0060), tensor(68.1637))]\n",
      "Epoch [13/25], Loss: 1.1769\n",
      "Epoch [13/25], Loss: 2.3994, Val_Accuracy [(tensor(64.0719), tensor(96.3074), tensor(68.0639))]\n",
      "Epoch [14/25], Loss: 1.0955\n",
      "Epoch [14/25], Loss: 3.5930, Val_Accuracy [(tensor(64.3713), tensor(97.0060), tensor(67.4651))]\n",
      "Epoch [15/25], Loss: 0.9267\n",
      "Epoch [15/25], Loss: 0.9989, Val_Accuracy [(tensor(64.7705), tensor(96.1078), tensor(65.2695))]\n",
      "Epoch [16/25], Loss: 1.4304\n",
      "Epoch [16/25], Loss: 3.8485, Val_Accuracy [(tensor(65.5689), tensor(96.5070), tensor(68.0639))]\n",
      "Epoch [17/25], Loss: 1.7094\n",
      "Epoch [17/25], Loss: 1.3906, Val_Accuracy [(tensor(65.5689), tensor(96.2076), tensor(66.5669))]\n",
      "Epoch [18/25], Loss: 1.5483\n",
      "Epoch [18/25], Loss: 3.9002, Val_Accuracy [(tensor(63.1737), tensor(96.1078), tensor(67.1657))]\n",
      "Epoch [19/25], Loss: 0.7959\n",
      "Epoch [19/25], Loss: 3.6546, Val_Accuracy [(tensor(64.1717), tensor(96.1078), tensor(66.1677))]\n",
      "Epoch [20/25], Loss: 0.8679\n",
      "Epoch [20/25], Loss: 1.0174, Val_Accuracy [(tensor(64.0719), tensor(96.8064), tensor(67.5649))]\n",
      "Epoch [21/25], Loss: 0.6626\n",
      "Epoch [21/25], Loss: 0.0037, Val_Accuracy [(tensor(64.0719), tensor(96.5070), tensor(66.5669))]\n",
      "Epoch [22/25], Loss: 0.1243\n",
      "Epoch [22/25], Loss: 2.8336, Val_Accuracy [(tensor(62.8743), tensor(96.1078), tensor(67.1657))]\n",
      "Epoch [23/25], Loss: 0.7269\n",
      "Epoch [23/25], Loss: 0.2713, Val_Accuracy [(tensor(63.7725), tensor(96.7066), tensor(65.3693))]\n",
      "Epoch [24/25], Loss: 0.0288\n",
      "Epoch [24/25], Loss: 3.3656, Val_Accuracy [(tensor(64.4711), tensor(96.4072), tensor(65.7685))]\n",
      "Epoch [25/25], Loss: 0.0656\n",
      "Epoch [25/25], Loss: 0.1009, Val_Accuracy [(tensor(63.1737), tensor(96.6068), tensor(64.9701))]\n",
      "7\n",
      "Net_separate(\n",
      "  (conv1_rgb): Conv2d(3, 32, kernel_size=(7, 7), stride=(1, 1))\n",
      "  (bn1_rgb): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2_rgb): Conv2d(32, 64, kernel_size=(7, 7), stride=(1, 1))\n",
      "  (bn2_rgb): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv4_rgb): Conv2d(64, 8, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (bn4_rgb): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc_rgb): Linear(in_features=4608, out_features=128, bias=True)\n",
      "  (conv1_mask): Conv2d(1, 32, kernel_size=(7, 7), stride=(1, 1))\n",
      "  (bn1_mask): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2_mask): Conv2d(32, 64, kernel_size=(7, 7), stride=(1, 1))\n",
      "  (bn2_mask): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv4_mask): Conv2d(64, 8, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (bn4_mask): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc_mask): Linear(in_features=4608, out_features=128, bias=True)\n",
      "  (conv1_bound): Conv2d(1, 32, kernel_size=(7, 7), stride=(1, 1))\n",
      "  (bn1_bound): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2_bound): Conv2d(32, 64, kernel_size=(7, 7), stride=(1, 1))\n",
      "  (bn2_bound): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv4_bound): Conv2d(64, 8, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (bn4_bound): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc_bound): Linear(in_features=4608, out_features=128, bias=True)\n",
      "  (conv1_normal): Conv2d(3, 32, kernel_size=(7, 7), stride=(1, 1))\n",
      "  (bn1_normal): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2_normal): Conv2d(32, 64, kernel_size=(7, 7), stride=(1, 1))\n",
      "  (bn2_normal): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv4_normal): Conv2d(64, 8, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (bn4_normal): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc_normal): Linear(in_features=4608, out_features=128, bias=True)\n",
      "  (fc1): Linear(in_features=512, out_features=8, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=8, bias=True)\n",
      "  (fc3): Linear(in_features=512, out_features=8, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25], Loss: 2.6016\n",
      "Epoch [1/25], Loss: 2.5801, Val_Accuracy [(tensor(36.7265), tensor(96.2076), tensor(51.9960))]\n",
      "Epoch [2/25], Loss: 2.5162\n",
      "Epoch [2/25], Loss: 2.6690, Val_Accuracy [(tensor(35.7285), tensor(96.2076), tensor(51.1976))]\n",
      "Epoch [3/25], Loss: 2.1282\n",
      "Epoch [3/25], Loss: 2.4770, Val_Accuracy [(tensor(37.0259), tensor(96.2076), tensor(53.3932))]\n",
      "Epoch [4/25], Loss: 1.4725\n",
      "Epoch [4/25], Loss: 2.6797, Val_Accuracy [(tensor(36.7265), tensor(96.2076), tensor(50.1996))]\n",
      "Epoch [5/25], Loss: 2.4318\n",
      "Epoch [5/25], Loss: 2.7054, Val_Accuracy [(tensor(38.2236), tensor(96.2076), tensor(51.7964))]\n",
      "Epoch [6/25], Loss: 2.7801\n",
      "Epoch [6/25], Loss: 1.7057, Val_Accuracy [(tensor(37.3253), tensor(96.2076), tensor(51.2974))]\n",
      "Epoch [7/25], Loss: 3.1452\n",
      "Epoch [7/25], Loss: 2.4411, Val_Accuracy [(tensor(37.9242), tensor(96.2076), tensor(50.8982))]\n",
      "Epoch [8/25], Loss: 2.9476\n",
      "Epoch [8/25], Loss: 2.8751, Val_Accuracy [(tensor(37.9242), tensor(96.2076), tensor(48.7026))]\n",
      "Epoch [9/25], Loss: 3.0005\n",
      "Epoch [9/25], Loss: 2.1021, Val_Accuracy [(tensor(38.0240), tensor(96.2076), tensor(52.6946))]\n",
      "Epoch [10/25], Loss: 3.1920\n",
      "Epoch [10/25], Loss: 2.9652, Val_Accuracy [(tensor(39.7206), tensor(96.2076), tensor(51.3972))]\n",
      "Epoch [11/25], Loss: 4.3963\n",
      "Epoch [11/25], Loss: 4.0373, Val_Accuracy [(tensor(38.1238), tensor(96.2076), tensor(49.0020))]\n",
      "Epoch [12/25], Loss: 2.1379\n",
      "Epoch [12/25], Loss: 3.6285, Val_Accuracy [(tensor(39.3214), tensor(96.2076), tensor(52.0958))]\n",
      "Epoch [13/25], Loss: 3.9364\n",
      "Epoch [13/25], Loss: 2.3381, Val_Accuracy [(tensor(38.7226), tensor(96.2076), tensor(52.6946))]\n",
      "Epoch [14/25], Loss: 2.3751\n",
      "Epoch [14/25], Loss: 1.8011, Val_Accuracy [(tensor(35.4291), tensor(96.2076), tensor(50.6986))]\n",
      "Epoch [15/25], Loss: 2.0689\n",
      "Epoch [15/25], Loss: 2.8388, Val_Accuracy [(tensor(36.4271), tensor(96.2076), tensor(50.7984))]\n",
      "Epoch [16/25], Loss: 3.5300\n",
      "Epoch [16/25], Loss: 1.7706, Val_Accuracy [(tensor(35.8283), tensor(96.2076), tensor(50.8982))]\n",
      "Epoch [17/25], Loss: 3.1778\n",
      "Epoch [17/25], Loss: 2.7041, Val_Accuracy [(tensor(37.6248), tensor(96.2076), tensor(52.6946))]\n",
      "Epoch [18/25], Loss: 2.2914\n",
      "Epoch [18/25], Loss: 3.6475, Val_Accuracy [(tensor(37.4251), tensor(96.2076), tensor(53.3932))]\n",
      "Epoch [19/25], Loss: 3.5569\n",
      "Epoch [19/25], Loss: 3.6095, Val_Accuracy [(tensor(39.0220), tensor(96.2076), tensor(51.2974))]\n",
      "Epoch [20/25], Loss: 3.1177\n",
      "Epoch [20/25], Loss: 1.8045, Val_Accuracy [(tensor(38.7226), tensor(96.2076), tensor(52.3952))]\n",
      "Epoch [21/25], Loss: 3.0511\n",
      "Epoch [21/25], Loss: 2.6740, Val_Accuracy [(tensor(37.2255), tensor(96.2076), tensor(50.1996))]\n",
      "Epoch [22/25], Loss: 1.9469\n",
      "Epoch [22/25], Loss: 2.8889, Val_Accuracy [(tensor(36.8263), tensor(96.2076), tensor(51.5968))]\n",
      "Epoch [23/25], Loss: 2.1807\n",
      "Epoch [23/25], Loss: 2.8812, Val_Accuracy [(tensor(38.7226), tensor(96.2076), tensor(54.8902))]\n",
      "Epoch [24/25], Loss: 3.0687\n",
      "Epoch [24/25], Loss: 2.1125, Val_Accuracy [(tensor(41.1178), tensor(96.2076), tensor(51.6966))]\n",
      "Epoch [25/25], Loss: 2.5022\n",
      "Epoch [25/25], Loss: 1.4732, Val_Accuracy [(tensor(36.5269), tensor(96.2076), tensor(49.9002))]\n"
     ]
    }
   ],
   "source": [
    "with open(\"results3.txt\", \"w\") as file: \n",
    "    for index, model in enumerate(list_of_models):\n",
    "        print( index)\n",
    "        #loss\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.0001,)\n",
    "        scheduler = StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "        print(model)\n",
    "        for epoch in range(max_epochs):\n",
    "\n",
    "            # Training\n",
    "            correct_azim = 0\n",
    "            correct_elev = 0\n",
    "            correct_inrot = 0\n",
    "            total = 0\n",
    "\n",
    "            model.train()\n",
    "            for i, (local_batch, local_labels) in enumerate(training_generator):\n",
    "\n",
    "                # Transfer to GPU\n",
    "                local_batch, local_labels = local_batch.to(device), local_labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # compute the model output\n",
    "                yaw ,pich,roll = model(local_batch[:, :lengthes[index]])\n",
    "\n",
    "                # calculate loss\n",
    "                train_loss = criterion(yaw, local_labels.long()[:,3])+ criterion(pich, local_labels.long()[:,4])+ criterion(roll, local_labels.long()[:,5])\n",
    "\n",
    "                # credit assignment\n",
    "                train_loss.backward()\n",
    "\n",
    "                # update model weights\n",
    "                optimizer.step() \n",
    "\n",
    "                # if (i+1) % 5 == 0:\n",
    "                    # print (f'Epoch [{epoch+1}/{max_epochs}], Step [{i+1}/{int(len(np.asarray(train_val_per_category[\"chair\"][\"train_set\"]))/10)}], Loss: {train_loss.item():.4f}')\n",
    "\n",
    "            file.write(f'Epoch [{epoch+1}/{max_epochs}], Loss: {train_loss.item():.4f}\\n') \n",
    "            print (f'Epoch [{epoch+1}/{max_epochs}], Loss: {train_loss.item():.4f}')\n",
    "\n",
    "            model.eval()    \n",
    "            for local_batch, local_labels in validation_generator:\n",
    "\n",
    "\n",
    "                # Transfer to GPU\n",
    "                local_batch, local_labels = local_batch.to(device), local_labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                yaw ,pich,roll= model(local_batch[:, :lengthes[index]])\n",
    "                #print(\"yaw\", yaw)\n",
    "                val_loss = criterion(yaw, local_labels.long()[:,3])+ criterion(pich, local_labels.long()[:,4])+ criterion(roll, local_labels.long()[:,5])\n",
    "                _, predicted_azim = torch.max(yaw.data, 1)\n",
    "                _, predicted_elev = torch.max(pich.data, 1)\n",
    "                _, predicted_inrot = torch.max(roll.data, 1)\n",
    "\n",
    "                total += local_labels.size(0)\n",
    "\n",
    "                if torch.cuda.is_available():\n",
    "\n",
    "                    correct_azim += (predicted_azim.cpu() == local_labels[:,3].cpu()).sum()\n",
    "                    correct_elev += (predicted_elev.cpu() == local_labels[:,4].cpu()).sum()\n",
    "                    correct_inrot += (predicted_inrot.cpu() == local_labels[:,5].cpu()).sum()\n",
    "\n",
    "                else:\n",
    "                    correct_azim += (predicted_azim == local_labels[:,3]).sum()\n",
    "                    correct_elev += (predicted_elev == local_labels[:,4]).sum()\n",
    "                    correct_inrot += (predicted_inrot == local_labels[:,5]).sum()\n",
    "\n",
    "\n",
    "                accuracy_azim = 100 * correct_azim / total\n",
    "                accuracy_elev = 100 * correct_elev / total\n",
    "                accuracy_inrot = 100 * correct_inrot / total\n",
    "            file.write(f'Epoch [{epoch+1}/{max_epochs}], Loss: {val_loss.item():.4f}, Val_Accuracy [{accuracy_azim.cpu(), accuracy_elev.cpu(), accuracy_inrot.cpu()}]\\n')\n",
    "            print (f'Epoch [{epoch+1}/{max_epochs}], Loss: {val_loss.item():.4f}, Val_Accuracy [{accuracy_azim.cpu(), accuracy_elev.cpu(), accuracy_inrot.cpu()}]')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simple_pose_estimation",
   "language": "python",
   "name": "simple_pose_estimation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
